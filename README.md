# ğŸ¥ Hospital Readmission Analytics Pipeline (Astronomer Orchestration)  

## ğŸ“Œ Overview  
This repository contains the **Airflow orchestration layer** of the Hospital Readmission Analytics pipeline, deployed on **Astronomer**.  

The DAGs here automate ingestion of hospital encounter data (simulated with Colab uploads to Snowflake), log audits, and trigger downstream dbt transformations.  

ğŸ‘‰ Note: The dbt models (`staging`, `fact_visits`, `dim_patients`, etc.) are stored in a **separate repo** dedicated to data modeling.  

---

## âš™ï¸ Tech Stack  
- **Data Source**: Kaggle Diabetes Hospital Readmission dataset  
- **Warehouse**: Snowflake (`RAW`, `STAGING`, `ANALYTICS`)  
- **Orchestration**: Apache Airflow on Astronomer  
- **Data Simulation**: Google Colab (daily chunk upload â†’ Snowflake Stage)  
- **Transformations**: dbt Core (in a separate repo)  
- **Docs & Versioning**: GitHub  

---

## ğŸš€ Pipeline Flow  

```mermaid
flowchart TD
  A[Colab Daily CSV Upload] -->|PUT to Stage| B[Snowflake RAW.HOSPITAL_STAGE]
  B -->|Airflow DAG COPY INTO| C[RAW.PATIENT_VISITS]
  C -->|Trigger dbt Cloud Job| D[STAGING + ANALYTICS Models]
  D -->|Power BI Connector| E[Dashboards & KPIs]
```

---

## ğŸ“‚ Repository Structure  

```
.
â”œâ”€â”€ .astro/                # Astronomer config
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ dags/                  # Airflow DAGs
â”‚   â”œâ”€â”€ daily_stage_loader.py
â”‚   â””â”€â”€ hello_world_dag.py
â”œâ”€â”€ Dockerfile             # Astronomer image build
â”œâ”€â”€ README.md              # (we just created this)
â”œâ”€â”€ packages.txt           # dbt Cloud packages (Astronomer pulls)
â””â”€â”€ requirements.txt       # Airflow Python deps
```

ğŸ‘‰ dbt project is located in a **separate repo**: `hospital_readmission_dbt`  

---

## âš™ï¸ Orchestration with Astronomer  

### DAG: `daily_stage_loader_http`  
1. **Truncate RAW** table.  
2. **Ingest yesterdayâ€™s file** from `RAW.HOSPITAL_STAGE`.  
3. **Validate row counts**.  
4. **Insert audit record** into `AUDIT.LOAD_LOGS`.  
5. **Trigger dbt Cloud job** (runs transformations in the dbt repo).  
6. **Update audit log** with fact counts.  
7. **Run dbt tests**.  

### DAG: `hello_world`  
- Simple test DAG with PythonOperator.  

---

## ğŸ§ª Data Quality & Validation  
- **dbt tests** (in the dbt repo):  
  - Uniqueness & not_null on IDs  
  - Accepted values for categorical fields  
  - Custom WARN if >5% Unknown demographics  

- **Audit logs (Snowflake)**:  
  - File name, execution date, RAW row counts, fact row counts, timestamps  

---

## ğŸ“Š BI Dashboard (Planned)  
Power BI connects directly to Snowflake `ANALYTICS` schema. Planned KPIs:  
- Readmission rate by diagnosis, age, admission type  
- Avg stay duration (readmitted vs not)  
- Patient volume trends  
- Filters for month, age, insurance type  

---

## âš ï¸ Roadblocks & Fixes  
- âŒ DAGs invisible in Airflow UI â†’ âœ… Confirmed via Astronomer DAGs tab & Grid View  
- âŒ Snowflake connector mismatch â†’ âœ… Fixed by pinning `snowflake-connector-python==3.9.1`  
- âŒ dbt privilege issues â†’ âœ… Fixed with proper grants to `HOSPITAL_ROLE`  
- âŒ dim_patients duplicates â†’ âœ… Fixed in dbt repo with â€œlatest encounter winsâ€ + window functions  
- âŒ Audit mismatches â†’ âœ… Fixed by logging fact row counts post-dbt  

---

## ğŸ“Œ Lessons Learned  
- Separate repos keep orchestration (Airflow) and transformations (dbt) clean.  
- Always align Snowflake connector versions with Airflow providers.  
- Simulation (Colab â†’ Snowflake stage) is a simple but effective way to model real-world feeds.  
- Audit logs are critical for debugging ingestion.  

---

## âš–ï¸ Next Steps  
- Add **Power BI dashboards** on top of Snowflake.  
- Extend dbt repo for predictive modeling (handling class imbalance with SMOTE & class weights).  

---

âœ¨ This repo demonstrates **Astronomer Airflow orchestration** of a healthcare data pipeline.  
